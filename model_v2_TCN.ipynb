{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "63--JizIqSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "P4QZZNJFrjDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "1yqyKgqqI7Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-tcn"
      ],
      "metadata": {
        "id": "6X6MV91QkE2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrpbxch7o4L_"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import yfinance as yf\n",
        "\n",
        "import optuna\n",
        "from pytorch_tcn.tcn import TCN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "aSgeqkTWLJRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Model and Dataset Classes\n",
        "# -------------------------------\n",
        "class TCNTimeSeries(nn.Module):\n",
        "    \"\"\"\n",
        "    A Time-Series model using a Temporal Convolutional Network (TCN).\n",
        "    Weâ€™ll map:\n",
        "      - input_size: number of input features (e.g., 4 for [Open, High, Low, Close])\n",
        "      - hidden_size: the channel size of the TCN layers\n",
        "      - num_layers: how many TCN layers to stack\n",
        "      - output_size: typically 1 for predicting a single value (e.g., 'Close')\n",
        "\n",
        "    You can adjust kernel_size, dropout, etc. as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, output_size=1,\n",
        "                 kernel_size=2, dropout=0.2):\n",
        "        super(TCNTimeSeries, self).__init__()\n",
        "        # e.g., if num_layers=3, and hidden_size=64, we get [64, 64, 64] channels\n",
        "        num_channels = [hidden_size] * num_layers\n",
        "\n",
        "        self.tcn = TCN(\n",
        "            num_inputs=input_size,\n",
        "            num_channels=num_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Expects x of shape (batch, sequence_length, input_size).\n",
        "        TCN wants (batch, input_channels, sequence_length), so transpose first.\n",
        "        \"\"\"\n",
        "        # x shape = (batch, seq_len, input_size)\n",
        "        x = x.transpose(1, 2)  # -> (batch, input_size, seq_len)\n",
        "\n",
        "        # Pass through TCN\n",
        "        y = self.tcn(x)  # shape -> (batch, hidden_size, seq_len)\n",
        "\n",
        "        # Take the last time step's output\n",
        "        y = y[:, :, -1]\n",
        "\n",
        "        # Linear layer to output final predictions\n",
        "        y = self.fc(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "SU5yR2WIqYmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPYDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]"
      ],
      "metadata": {
        "id": "tHwWYWxVqaFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Helper Functions\n",
        "# -------------------------------\n",
        "def create_sequences(data, window_size=30, target_step=1):\n",
        "    X, y = [], []\n",
        "    num_samples = len(data)\n",
        "    for i in range(num_samples - window_size - target_step + 1):\n",
        "        X_seq = data[i : i + window_size]\n",
        "        y_seq = data[i + window_size + target_step - 1]\n",
        "        X.append(X_seq)\n",
        "        y.append(y_seq[3])  # Only predict 'Close' (index = 3)\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "j5IlowMlqbtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_for_tuning(ticker, start_date, end_date,\n",
        "                         sequence_length=30,\n",
        "                         prediction_days=1,\n",
        "                         val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Fetch data, create sequences, and split into train/val/test sets.\n",
        "    val_ratio and test_ratio are fractions of the entire dataset used for validation and test.\n",
        "    \"\"\"\n",
        "    # Fetch data\n",
        "    spy_df = yf.download(ticker, start=start_date, end=end_date)\n",
        "    spy_df.reset_index(inplace=True)\n",
        "\n",
        "    features = ['Open', 'High', 'Low', 'Close']\n",
        "    data = spy_df[features].values\n",
        "\n",
        "    # Scale data\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "    # Create sequences\n",
        "    X_all, y_all = create_sequences(data_scaled, window_size=sequence_length, target_step=prediction_days)\n",
        "\n",
        "    # Train/val/test split\n",
        "    dataset_size = len(X_all)\n",
        "    val_size = int(val_ratio * dataset_size)\n",
        "    test_size = int(test_ratio * dataset_size)\n",
        "    train_size = dataset_size - val_size - test_size\n",
        "\n",
        "    X_train, X_val, X_test = X_all[:train_size], X_all[train_size:train_size+val_size], X_all[train_size+val_size:]\n",
        "    y_train, y_val, y_test = y_all[:train_size], y_all[train_size:train_size+val_size], y_all[train_size+val_size:]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler, data"
      ],
      "metadata": {
        "id": "wHos3ffQJJNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Objective Function for Optuna\n",
        "# -------------------------------\n",
        "def objective(trial, ticker, start_date, end_date,\n",
        "              sequence_length=30,\n",
        "              prediction_days=1,\n",
        "              n_epochs=10,\n",
        "              val_ratio=0.1,\n",
        "              test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Objective function that Optuna will call multiple times,\n",
        "    each time sampling different hyperparameters for the TCN.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1) Suggest hyperparameters\n",
        "    # -----------------------------\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, step=32)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
        "\n",
        "    # Optional: also tune kernel_size or dropout\n",
        "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2) Load data\n",
        "    # -----------------------------\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, scaler, data = load_data_for_tuning(\n",
        "        ticker, start_date, end_date,\n",
        "        sequence_length=sequence_length,\n",
        "        prediction_days=prediction_days,\n",
        "        val_ratio=val_ratio,\n",
        "        test_ratio=test_ratio\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3) Create DataLoaders\n",
        "    # -----------------------------\n",
        "    train_dataset = SPYDataset(X_train, y_train)\n",
        "    val_dataset   = SPYDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4) Define model, loss, opt\n",
        "    # -----------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 4  # ['Open', 'High', 'Low', 'Close']\n",
        "    output_size = 1\n",
        "\n",
        "    # Use the TCN model\n",
        "    model = TCNTimeSeries(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        output_size=output_size,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5) Train\n",
        "    # -----------------------------\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6) Validation\n",
        "    # -----------------------------\n",
        "    model.eval()\n",
        "    val_predictions, val_actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            val_predictions.append(outputs.cpu().numpy())\n",
        "            val_actuals.append(y_batch.numpy().reshape(-1, 1))\n",
        "\n",
        "    val_predictions = np.vstack(val_predictions)\n",
        "    val_actuals = np.vstack(val_actuals)\n",
        "\n",
        "    # Invert scaling for 'Close' (index=3)\n",
        "    pred_close_scaled = np.zeros((len(val_predictions), data.shape[1]))\n",
        "    act_close_scaled = np.zeros((len(val_actuals), data.shape[1]))\n",
        "\n",
        "    pred_close_scaled[:, 3] = val_predictions.flatten()\n",
        "    act_close_scaled[:, 3] = val_actuals.flatten()\n",
        "\n",
        "    pred_close = scaler.inverse_transform(pred_close_scaled)[:, 3]\n",
        "    act_close  = scaler.inverse_transform(act_close_scaled)[:, 3]\n",
        "\n",
        "    rmse_val = float(np.sqrt(np.mean((pred_close - act_close) ** 2)))\n",
        "    return rmse_val"
      ],
      "metadata": {
        "id": "pJaEZoNGJPdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Run Optuna Tuning\n",
        "# -------------------------------\n",
        "def tune_hyperparams(ticker=\"SPY\",\n",
        "                     start_date=\"2000-01-01\",\n",
        "                     end_date=\"2025-01-01\",\n",
        "                     sequence_length=30,\n",
        "                     prediction_days=1,\n",
        "                     n_epochs=10,\n",
        "                     val_ratio=0.1,\n",
        "                     test_ratio=0.1,\n",
        "                     n_trials=20):\n",
        "    \"\"\"\n",
        "    Runs an Optuna study to tune hyperparameters of the TCN model.\n",
        "    Returns the best hyperparameters.\n",
        "    \"\"\"\n",
        "    def optuna_objective(trial):\n",
        "        return objective(\n",
        "            trial,\n",
        "            ticker=ticker,\n",
        "            start_date=start_date,\n",
        "            end_date=end_date,\n",
        "            sequence_length=sequence_length,\n",
        "            prediction_days=prediction_days,\n",
        "            n_epochs=n_epochs,\n",
        "            val_ratio=val_ratio,\n",
        "            test_ratio=test_ratio\n",
        "        )\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(optuna_objective, n_trials=n_trials)\n",
        "\n",
        "    print(\"Best trial:\", study.best_trial)\n",
        "    print(\"Best RMSE:\", study.best_value)\n",
        "    print(\"Best hyperparameters:\", study.best_params)\n",
        "    return study.best_params"
      ],
      "metadata": {
        "id": "1UgIBBKjJQhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Predict & Forecast Function\n",
        "# -------------------------------\n",
        "def predict_spy(ticker, start_date, end_date,\n",
        "                prediction_days=1,\n",
        "                sequence_length=30,\n",
        "                epochs=10,\n",
        "                forecast_days=5,\n",
        "                # Optionally pass tuned params or use defaults\n",
        "                hidden_size=64,\n",
        "                num_layers=1,\n",
        "                lr=1e-3,\n",
        "                batch_size=32,\n",
        "                kernel_size=2,\n",
        "                dropout=0.2):\n",
        "    \"\"\"\n",
        "    Main function to fetch data from Yahoo Finance, train the TCN, and extend predictions to future days.\n",
        "    Optionally uses hyperparameters from Optuna (if you supply them).\n",
        "    \"\"\"\n",
        "    spy_df = yf.download(ticker, start=start_date, end=end_date)\n",
        "    spy_df.reset_index(inplace=True)\n",
        "\n",
        "    features = ['Open', 'High', 'Low', 'Close']\n",
        "    data = spy_df[features].values\n",
        "\n",
        "    # Scale data\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "    # Create sequences\n",
        "    X_all, y_all = create_sequences(data_scaled,\n",
        "                                    window_size=sequence_length,\n",
        "                                    target_step=prediction_days)\n",
        "\n",
        "    # Train/test split (80/20)\n",
        "    train_size = int(0.8 * len(X_all))\n",
        "    X_train, X_test = X_all[:train_size], X_all[train_size:]\n",
        "    y_train, y_test = y_all[:train_size], y_all[train_size:]\n",
        "\n",
        "    # Datasets & Loaders\n",
        "    train_dataset = SPYDataset(X_train, y_train)\n",
        "    test_dataset  = SPYDataset(X_test,  y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = len(features)\n",
        "    output_size = 1\n",
        "\n",
        "    # Initialize TCN\n",
        "    model = TCNTimeSeries(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        output_size=output_size,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * X_batch.size(0)\n",
        "        epoch_loss /= len(train_loader.dataset)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    # Predictions on test set\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            actuals.append(y_batch.numpy().reshape(-1, 1))\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    actuals     = np.vstack(actuals)\n",
        "\n",
        "    # Invert scaling (Close only, index=3)\n",
        "    pred_close_scaled = np.zeros((len(predictions), data.shape[1]))\n",
        "    act_close_scaled  = np.zeros((len(actuals), data.shape[1]))\n",
        "\n",
        "    pred_close_scaled[:, 3] = predictions.flatten()\n",
        "    act_close_scaled[:, 3]  = actuals.flatten()\n",
        "\n",
        "    pred_close = scaler.inverse_transform(pred_close_scaled)[:, 3]\n",
        "    act_close  = scaler.inverse_transform(act_close_scaled)[:, 3]\n",
        "\n",
        "    mse  = np.mean((pred_close - act_close) ** 2)\n",
        "    rmse = float(np.sqrt(mse))\n",
        "\n",
        "    # Forecast future prices\n",
        "    future_predictions = []\n",
        "    last_sequence = data_scaled[-sequence_length:]  # Last available sequence\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(forecast_days):\n",
        "            last_seq_tensor = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            next_scaled_price = model(last_seq_tensor).cpu().numpy()\n",
        "\n",
        "            # Insert into a zero vector so we can invert scale\n",
        "            next_scaled_price_full = np.zeros((1, data.shape[1]))\n",
        "            next_scaled_price_full[0, 3] = next_scaled_price\n",
        "            next_price = scaler.inverse_transform(next_scaled_price_full)[0, 3]\n",
        "\n",
        "            future_predictions.append(next_price)\n",
        "\n",
        "            # Shift sequence and add new predicted close\n",
        "            next_sequence = np.roll(last_sequence, -1, axis=0)\n",
        "            next_sequence[-1, 3] = next_scaled_price\n",
        "            last_sequence = next_sequence\n",
        "\n",
        "    # Combine predictions and future forecasts for plotting\n",
        "    forecast_indices = np.arange(len(act_close), len(act_close) + forecast_days)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(y=act_close, mode='lines', name='Actual Close'))\n",
        "    fig.add_trace(go.Scatter(y=pred_close, mode='lines', name='Predicted Close'))\n",
        "    fig.add_trace(go.Scatter(x=forecast_indices, y=future_predictions,\n",
        "                             mode='lines', name='Forecasted Prices',\n",
        "                             line=dict(dash='dot')))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'{ticker} Actual, Predicted, and Forecasted Close Prices (TCN)',\n",
        "        xaxis_title='Index',\n",
        "        yaxis_title='Price'\n",
        "    )\n",
        "\n",
        "    return fig, rmse"
      ],
      "metadata": {
        "id": "4Kg3w0lEqeEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = \"SPY\"\n",
        "start_date=\"2019-01-24\"\n",
        "end_date=\"2025-01-28\"\n",
        "prediction_days=1"
      ],
      "metadata": {
        "id": "l6SYKBPRKrmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best hyperparameters\n",
        "best_params = tune_hyperparams(\n",
        "    ticker=ticker,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    sequence_length=30,\n",
        "    prediction_days=prediction_days,\n",
        "    n_epochs=10,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    n_trials=20\n",
        ")\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-MqTEGnJnP4",
        "outputId": "5da46a0a-1684-4e13-85f7-0734699aa24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-28 20:22:30,618] A new study created in memory with name: no-name-7a67093b-0bac-4852-ae69-5a3c51983de7\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:31,924] Trial 0 finished with value: 10.223791024805307 and parameters: {'hidden_size': 224, 'num_layers': 1, 'lr': 0.0016469944787488161, 'batch_size': 64, 'kernel_size': 5, 'dropout': 0.45823468368555675}. Best is trial 0 with value: 10.223791024805307.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:32,373] Trial 1 finished with value: 10.725399841206666 and parameters: {'hidden_size': 192, 'num_layers': 1, 'lr': 0.0017584683762329324, 'batch_size': 128, 'kernel_size': 3, 'dropout': 0.36115977973192326}. Best is trial 0 with value: 10.223791024805307.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:33,067] Trial 2 finished with value: 42.226262509786935 and parameters: {'hidden_size': 32, 'num_layers': 1, 'lr': 0.0002779231469149559, 'batch_size': 64, 'kernel_size': 3, 'dropout': 0.3140789731505894}. Best is trial 0 with value: 10.223791024805307.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:34,280] Trial 3 finished with value: 44.702915052670576 and parameters: {'hidden_size': 32, 'num_layers': 1, 'lr': 0.00028414810759057183, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.45213678442646255}. Best is trial 0 with value: 10.223791024805307.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:35,402] Trial 4 finished with value: 21.51600939585032 and parameters: {'hidden_size': 192, 'num_layers': 3, 'lr': 0.00012419042511502948, 'batch_size': 128, 'kernel_size': 4, 'dropout': 0.24066628298172688}. Best is trial 0 with value: 10.223791024805307.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:37,111] Trial 5 finished with value: 5.529206910331779 and parameters: {'hidden_size': 32, 'num_layers': 2, 'lr': 0.0018477661681046377, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.028230525180696353}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:39,047] Trial 6 finished with value: 11.134238010005172 and parameters: {'hidden_size': 96, 'num_layers': 2, 'lr': 0.0015274382984197157, 'batch_size': 32, 'kernel_size': 3, 'dropout': 0.13295214766213265}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:40,051] Trial 7 finished with value: 24.641912535499493 and parameters: {'hidden_size': 32, 'num_layers': 2, 'lr': 0.0004367538054578062, 'batch_size': 64, 'kernel_size': 2, 'dropout': 0.3022198174446281}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:42,179] Trial 8 finished with value: 34.26328286596548 and parameters: {'hidden_size': 96, 'num_layers': 3, 'lr': 0.006849392532736214, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.25432040318645405}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:45,155] Trial 9 finished with value: 13.062099775767464 and parameters: {'hidden_size': 224, 'num_layers': 2, 'lr': 0.009399194115884807, 'batch_size': 16, 'kernel_size': 5, 'dropout': 0.2488537699063113}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:49,015] Trial 10 finished with value: 25.375095789798923 and parameters: {'hidden_size': 96, 'num_layers': 3, 'lr': 0.0034589279630588016, 'batch_size': 16, 'kernel_size': 4, 'dropout': 0.0043481854802497125}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:49,700] Trial 11 finished with value: 36.57124380542487 and parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0008220000029619301, 'batch_size': 64, 'kernel_size': 5, 'dropout': 0.4957227426463838}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:50,603] Trial 12 finished with value: 16.643721726816835 and parameters: {'hidden_size': 160, 'num_layers': 2, 'lr': 0.0032718558781483537, 'batch_size': 64, 'kernel_size': 4, 'dropout': 0.06511387057247758}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:51,828] Trial 13 finished with value: 6.019005260245646 and parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0007936783663008674, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.15461956738811733}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:53,446] Trial 14 finished with value: 6.311880189791032 and parameters: {'hidden_size': 128, 'num_layers': 2, 'lr': 0.0007606041838672415, 'batch_size': 32, 'kernel_size': 4, 'dropout': 0.12926264355888117}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:54,607] Trial 15 finished with value: 18.479826088742826 and parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0032092779596218754, 'batch_size': 32, 'kernel_size': 2, 'dropout': 0.14611219670297748}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:56,679] Trial 16 finished with value: 15.323418919912513 and parameters: {'hidden_size': 64, 'num_layers': 3, 'lr': 0.00046139293170580347, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.008180513341121805}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:58,310] Trial 17 finished with value: 13.781994553832817 and parameters: {'hidden_size': 160, 'num_layers': 2, 'lr': 0.0011248485822934896, 'batch_size': 32, 'kernel_size': 4, 'dropout': 0.08599425280008474}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:22:59,474] Trial 18 finished with value: 25.244399876967904 and parameters: {'hidden_size': 128, 'num_layers': 1, 'lr': 0.00012566076207907767, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.1872391728499956}. Best is trial 5 with value: 5.529206910331779.\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[I 2025-01-28 20:23:02,475] Trial 19 finished with value: 7.573259646802798 and parameters: {'hidden_size': 64, 'num_layers': 2, 'lr': 0.0023831969532756817, 'batch_size': 16, 'kernel_size': 4, 'dropout': 0.05926526550894029}. Best is trial 5 with value: 5.529206910331779.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial: FrozenTrial(number=5, state=1, values=[5.529206910331779], datetime_start=datetime.datetime(2025, 1, 28, 20, 22, 35, 403105), datetime_complete=datetime.datetime(2025, 1, 28, 20, 22, 37, 110866), params={'hidden_size': 32, 'num_layers': 2, 'lr': 0.0018477661681046377, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.028230525180696353}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'hidden_size': IntDistribution(high=256, log=False, low=32, step=32), 'num_layers': IntDistribution(high=3, log=False, low=1, step=1), 'lr': FloatDistribution(high=0.01, log=True, low=0.0001, step=None), 'batch_size': CategoricalDistribution(choices=(16, 32, 64, 128)), 'kernel_size': IntDistribution(high=5, log=False, low=2, step=1), 'dropout': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=5, value=None)\n",
            "Best RMSE: 5.529206910331779\n",
            "Best hyperparameters: {'hidden_size': 32, 'num_layers': 2, 'lr': 0.0018477661681046377, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.028230525180696353}\n",
            "{'hidden_size': 32, 'num_layers': 2, 'lr': 0.0018477661681046377, 'batch_size': 32, 'kernel_size': 5, 'dropout': 0.028230525180696353}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass into prediction (you can add kernel_size, dropout, etc. if they were tuned)\n",
        "fig, rmse = predict_spy(\n",
        "    ticker=ticker,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    prediction_days=prediction_days,\n",
        "    sequence_length=30,\n",
        "    epochs=10,\n",
        "    forecast_days=5,\n",
        "    hidden_size=best_params[\"hidden_size\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    lr=best_params[\"lr\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    kernel_size=best_params.get(\"kernel_size\", 2),\n",
        "    dropout=best_params.get(\"dropout\", 0.2)\n",
        ")\n",
        "print(\"Final Test RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxclgw5NJtpD",
        "outputId": "3d83191e-53c6-4a1d-fd76-0b8d9c55c363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.003960\n",
            "Epoch [2/10], Loss: 0.000445\n",
            "Epoch [3/10], Loss: 0.000414\n",
            "Epoch [4/10], Loss: 0.000363\n",
            "Epoch [5/10], Loss: 0.000296\n",
            "Epoch [6/10], Loss: 0.000309\n",
            "Epoch [7/10], Loss: 0.000263\n",
            "Epoch [8/10], Loss: 0.000241\n",
            "Epoch [9/10], Loss: 0.000224\n",
            "Epoch [10/10], Loss: 0.000203\n",
            "Final Test RMSE: 24.372614388119963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Gradio Interface\n",
        "# -------------------------------\n",
        "demo = gr.Interface(\n",
        "    fn=predict_spy,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Ticker Symbol\", placeholder=\"e.g., SPY\", value=\"SPY\"),\n",
        "        gr.Textbox(label=\"Start Date\", value=\"2019-01-24\"),\n",
        "        gr.Textbox(label=\"End Date\", value=\"2025-01-28\"),\n",
        "        gr.Slider(label=\"Prediction Days Ahead\", minimum=1, maximum=30, value=1, step=1),\n",
        "        gr.Slider(label=\"Sequence Length\", minimum=10, maximum=200, value=60, step=5),\n",
        "        gr.Slider(label=\"Number of Epochs\", minimum=1, maximum=50, value=50, step=1),\n",
        "        gr.Slider(label=\"Forecast Days\", minimum=1, maximum=60, value=30, step=1)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Plot(label=\"Predictions and Forecast\"),\n",
        "        gr.Number(label=\"RMSE\")\n",
        "    ],\n",
        "    description=(\n",
        "        \"Enter a stock ticker symbol (e.g., SPY) to fetch historical data from Yahoo Finance. \"\n",
        "        \"Adjust the date range, prediction days, sequence length, and training epochs. \"\n",
        "        \"The model predicts and forecasts future prices.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "juN-ZT3gKP0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run hyperparameter tuning first (optional)\n",
        "    # Then pass them into predict_spy (for instance) in your actual usage.\n",
        "\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "RiZxQX5gJZKy",
        "outputId": "0ee85991-86b0-4782-c6cc-d6bad2865290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6e085c2ce99a1eb116.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6e085c2ce99a1eb116.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}